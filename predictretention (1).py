# -*- coding: utf-8 -*-
"""PredictRetention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16E5Xfot7Kw3b2TZchTH3s_s0wJx8MXbU
"""

from google.colab import files
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score
import pandas as pd

# Step 1: Load CSV data and adjust column names
# Replace with your actual file path
file_path = '/content/PredictionRetention - Sheet11.csv'

# Read CSV with header=0 to automatically use the first row as column names
df = pd.read_csv(file_path, header=0)

# Display the first few rows to check data and column names
# print("Initial DataFrame:\n", df.head())

# Rename columns for easier handling
df.columns = ['UserName', 'Time', 'game_name', 'GameExited', 'Genre']

# Step 2: Preprocess data
df['Time'] = pd.to_datetime(df['Time'], errors='coerce')

# Drop rows where 'Time' could not be parsed
df = df.dropna(subset=['Time'])

# Step 3: Calculate features


def calculate_features(df):
    # Extract hour for peak activity
    df['hour'] = df['Time'].dt.hour

    # Aggregate features by UserName
    user_features = df.groupby('UserName').agg(
        D0_total_time=('GameExited', 'sum'),  # Total time spent on D0
        # Number of unique games played on D0
        D0_num_games=('game_name', 'nunique'),
        D0_num_sessions=('Time', 'count'),  # Number of distinct sessions on D0
        # Number of unique genres played on D0
        D0_num_genres=('Genre', 'nunique'),
        D0_most_played_genre=('Genre', lambda x: x.mode()[
                              0] if not x.mode().empty else None),  # Most played genre on D0
        D0_most_played_game=('game_name', lambda x: x.mode()[
                             0] if not x.mode().empty else None),  # Most played game on D0
        D0_first_activity_hour=('hour', 'min'),  # First activity hour on D0
        D0_last_activity_hour=('hour', 'max'),  # Last activity hour on D0
        # Average session duration on D0
        D0_avg_session_duration=('GameExited', 'mean'),
        D0_peak_activity_hour=('hour', lambda x: x.value_counts().idxmax(
        ) if not x.value_counts().empty else None)  # Peak activity hour on D0
    ).reset_index()

    return user_features


# Calculate features
user_features = calculate_features(df)

# Save the features to a CSV file
user_features.to_csv('user_features.csv', index=False)


# Load dataset
# Replace with your actual file path
file_path = '/content/PredictionRetention - Dataset2 (4).csv'
data = pd.read_csv(file_path)

# Encode categorical features
label_encoder = LabelEncoder()
data['D0_most_played_genre'] = label_encoder.fit_transform(
    data['D0_most_played_genre'])
data['D0_most_played_game'] = label_encoder.fit_transform(
    data['D0_most_played_game'])

# Define input features and output labels
features = ['D0_total_time', 'D0_num_games', 'D0_num_sessions', 'D0_num_genres', 'D0_most_played_genre',
            'D0_most_played_game', 'D0_first_activity_hour', 'D0_last_activity_hour']
X = data[features]
y_d1 = data['D1']
y_d2 = data['D2']
y_d3 = data['D3']
y_d6 = data['D6']

# Split data into training and testing sets
X_train_d1, X_test_d1, y_train_d1, y_test_d1 = train_test_split(
    X, y_d1, test_size=0.2, random_state=42)
X_train_d2, X_test_d2, y_train_d2, y_test_d2 = train_test_split(
    X, y_d2, test_size=0.2, random_state=42)
X_train_d3, X_test_d3, y_train_d3, y_test_d3 = train_test_split(
    X, y_d3, test_size=0.2, random_state=42)
X_train_d6, X_test_d6, y_train_d6, y_test_d6 = train_test_split(
    X, y_d6, test_size=0.2, random_state=42)


# Train and evaluate Random Forest models

def train_and_evaluate_random_forest(X_train, X_test, y_train, y_test, day):
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"--- Random Forest Retention D{day} ---")
    print(classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))
    return model, model.feature_importances_


rf_model_d1, rf_importances_d1 = train_and_evaluate_random_forest(
    X_train_d1, X_test_d1, y_train_d1, y_test_d1, 1)
rf_model_d2, rf_importances_d2 = train_and_evaluate_random_forest(
    X_train_d2, X_test_d2, y_train_d2, y_test_d2, 2)
rf_model_d3, rf_importances_d3 = train_and_evaluate_random_forest(
    X_train_d3, X_test_d3, y_train_d3, y_test_d3, 3)
rf_model_d6, rf_importances_d6 = train_and_evaluate_random_forest(
    X_train_d6, X_test_d6, y_train_d6, y_test_d6, 6)

# Train and evaluate Gradient Boosting models


def train_and_evaluate_gradient_boosting(X_train, X_test, y_train, y_test, day):
    model = GradientBoostingClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"--- Gradient Boosting Retention D{day} ---")
    # print(classification_report(y_test, y_pred))
    # print("Accuracy:", accuracy_score(y_test, y_pred))
    return model, model.feature_importances_


gb_model_d1, gb_importances_d1 = train_and_evaluate_gradient_boosting(
    X_train_d1, X_test_d1, y_train_d1, y_test_d1, 1)
gb_model_d2, gb_importances_d2 = train_and_evaluate_gradient_boosting(
    X_train_d2, X_test_d2, y_train_d2, y_test_d2, 2)
gb_model_d3, gb_importances_d3 = train_and_evaluate_gradient_boosting(
    X_train_d3, X_test_d3, y_train_d3, y_test_d3, 3)
gb_model_d6, gb_importances_d6 = train_and_evaluate_gradient_boosting(
    X_train_d6, X_test_d6, y_train_d6, y_test_d6, 6)

# Combine feature importances into a DataFrame
importances_df = pd.DataFrame({
    'Feature': features,
    'RF_Importances_D1': rf_importances_d1,
    'RF_Importances_D2': rf_importances_d2,
    'RF_Importances_D3': rf_importances_d3,
    'RF_Importances_D6': rf_importances_d6,
    'GB_Importances_D1': gb_importances_d1,
    'GB_Importances_D2': gb_importances_d2,
    'GB_Importances_D3': gb_importances_d3,
    'GB_Importances_D6': gb_importances_d6,
})

print(importances_df)


# Load dataset
# Replace with your actual file path
file_path = '/content/PredictionRetention - Dataset2 (4).csv'
data = pd.read_csv(file_path)

# Encode categorical features
label_encoder = LabelEncoder()
data['D0_most_played_genre'] = label_encoder.fit_transform(
    data['D0_most_played_genre'])
data['D0_most_played_game'] = label_encoder.fit_transform(
    data['D0_most_played_game'])


# Define input features and output labels
features = ['D0_total_time', 'D0_num_games', 'D0_num_sessions', 'D0_num_genres', 'D0_most_played_genre',
            'D0_most_played_game', 'D0_first_activity_hour', 'D0_last_activity_hour']

# # Convert categorical features to dummy variables if needed
# data = pd.get_dummies(data, columns=['D0_most_played_genre', 'D0_most_played_game'], drop_first=True)

# Include output labels for correlation
correlation_data = data[features + ['D1', 'D2', 'D3', 'D6']]

# Compute correlation matrix
corr_matrix = correlation_data.corr()

# Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm',
            fmt=".2f", annot_kws={"size": 10})
plt.title('Correlation Heatmap between Features and Output Labels')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

new_data = pd.DataFrame({
    'D0_total_time': [3.6],  # Replace with actual new data
    'D0_num_games': [1],
    'D0_num_sessions': [0],
    'D0_num_genres': [1],
    'D0_most_played_genre': [1],
    'D0_most_played_game': [3],
    'D0_first_activity_hour': [20],
    'D0_last_activity_hour': [20]
})

predictions_rf_D1 = rf_model_d1.predict(new_data)
predictions_rf_D2 = rf_model_d2.predict(new_data)
predictions_rf_D3 = rf_model_d3.predict(new_data)
predictions_rf_D6 = rf_model_d6.predict(new_data)


print("Random Forest Predictions for new data (D1):", predictions_rf_D1)
print("Random Forest Predictions for new data (D2):", predictions_rf_D2)
print("Random Forest Predictions for new data (D3):", predictions_rf_D3)
print("Random Forest Predictions for new data (D6):", predictions_rf_D6)

# Replace with the path to your new data CSV file
new_data_file_path = '/content/testing.csv'
new_data = pd.read_csv(new_data_file_path)

# Ensure the new data has the same feature columns as the training data
new_data_features = new_data[features]

# Step 3: Predict retention for the new data using Random Forest models
new_data['RF_Pred_D1'] = rf_model_d1.predict(new_data_features)
new_data['RF_Pred_D2'] = rf_model_d2.predict(new_data_features)
new_data['RF_Pred_D3'] = rf_model_d3.predict(new_data_features)
new_data['RF_Pred_D6'] = rf_model_d6.predict(new_data_features)

# Step 4: Save the predictions to a new CSV file
output_file_path = 'predicted_user_retention.csv'
new_data.to_csv(output_file_path, index=False)

# print(f"Predictions saved to {output_file_path}")

pickle.dump(rf_model_d1, open('rf_model_d1.pkl', 'wb'))
pickle.dump(rf_model_d2, open('rf_model_d2.pkl', 'wb'))
pickle.dump(rf_model_d3, open('rf_model_d3.pkl', 'wb'))
pickle.dump(rf_model_d6, open('rf_model_d6.pkl', 'wb'))

rf_model_d1 = pickle.load(open('rf_model_d1.pkl', 'rb'))
print(rf_model_d1.predict([[8, 4, 0, 3, 4, 2, 11, 11]]))
rf_model_d2 = pickle.load(open('rf_model_d2.pkl', 'rb'))
print(rf_model_d2.predict([[8, 4, 0, 3, 4, 2, 11, 11]]))


# For a scikit-learn model
files.download('rf_model_d1.pkl')
files.download('rf_model_d2.pkl')
files.download('rf_model_d3.pkl')

files.download('rf_model_d6.pkl')
